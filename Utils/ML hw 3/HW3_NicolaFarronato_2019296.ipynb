{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "We consider the same notebook used in the labs, containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: Insert your ID number (\"numero di matricola\") below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put here your ``numero di matricola''\n",
    "numero_di_matricola = 2019296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all packages needed\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data, remove data samples/points with missing values (NaN), and print some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.645240e+09</td>\n",
       "      <td>5.354358e+05</td>\n",
       "      <td>3.381163</td>\n",
       "      <td>2.071903</td>\n",
       "      <td>2070.027813</td>\n",
       "      <td>1.525054e+04</td>\n",
       "      <td>1.434893</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.244311</td>\n",
       "      <td>3.459229</td>\n",
       "      <td>7.615676</td>\n",
       "      <td>1761.252212</td>\n",
       "      <td>308.775601</td>\n",
       "      <td>1967.489254</td>\n",
       "      <td>94.668774</td>\n",
       "      <td>98077.125158</td>\n",
       "      <td>47.557868</td>\n",
       "      <td>-122.212337</td>\n",
       "      <td>1982.544564</td>\n",
       "      <td>13176.302465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.854203e+09</td>\n",
       "      <td>3.809004e+05</td>\n",
       "      <td>0.895472</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>920.251879</td>\n",
       "      <td>4.254457e+04</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.098513</td>\n",
       "      <td>0.776298</td>\n",
       "      <td>0.682592</td>\n",
       "      <td>1.166324</td>\n",
       "      <td>815.934864</td>\n",
       "      <td>458.977904</td>\n",
       "      <td>28.095275</td>\n",
       "      <td>424.439427</td>\n",
       "      <td>54.172937</td>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>686.256670</td>\n",
       "      <td>25413.180755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.490000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.177500</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.199775e+09</td>\n",
       "      <td>3.150000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>5.453750e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98032.000000</td>\n",
       "      <td>47.459575</td>\n",
       "      <td>-122.324250</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>5429.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.027701e+09</td>\n",
       "      <td>4.450000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>8.000000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1545.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1969.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98059.000000</td>\n",
       "      <td>47.572500</td>\n",
       "      <td>-122.226000</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>7873.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.358175e+09</td>\n",
       "      <td>6.402500e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.122250e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2150.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98117.000000</td>\n",
       "      <td>47.680250</td>\n",
       "      <td>-122.124000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10408.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.839301e+09</td>\n",
       "      <td>5.350000e+06</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8010.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6720.000000</td>\n",
       "      <td>2620.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.000000</td>\n",
       "      <td>425581.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price     bedrooms    bathrooms  sqft_living  \\\n",
       "count  3.164000e+03  3.164000e+03  3164.000000  3164.000000  3164.000000   \n",
       "mean   4.645240e+09  5.354358e+05     3.381163     2.071903  2070.027813   \n",
       "std    2.854203e+09  3.809004e+05     0.895472     0.768212   920.251879   \n",
       "min    1.000102e+06  7.500000e+04     0.000000     0.000000   380.000000   \n",
       "25%    2.199775e+09  3.150000e+05     3.000000     1.500000  1430.000000   \n",
       "50%    4.027701e+09  4.450000e+05     3.000000     2.000000  1910.000000   \n",
       "75%    7.358175e+09  6.402500e+05     4.000000     2.500000  2500.000000   \n",
       "max    9.839301e+09  5.350000e+06     8.000000     6.000000  8010.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  3.164000e+03  3164.000000  3164.000000  3164.000000  3164.000000   \n",
       "mean   1.525054e+04     1.434893     0.009798     0.244311     3.459229   \n",
       "std    4.254457e+04     0.507792     0.098513     0.776298     0.682592   \n",
       "min    6.490000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.453750e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    8.000000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.122250e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above  sqft_basement     yr_built  yr_renovated  \\\n",
       "count  3164.000000  3164.000000    3164.000000  3164.000000   3164.000000   \n",
       "mean      7.615676  1761.252212     308.775601  1967.489254     94.668774   \n",
       "std       1.166324   815.934864     458.977904    28.095275    424.439427   \n",
       "min       3.000000   380.000000       0.000000  1900.000000      0.000000   \n",
       "25%       7.000000  1190.000000       0.000000  1950.000000      0.000000   \n",
       "50%       7.000000  1545.000000       0.000000  1969.000000      0.000000   \n",
       "75%       8.000000  2150.000000     600.000000  1990.000000      0.000000   \n",
       "max      12.000000  6720.000000    2620.000000  2015.000000   2015.000000   \n",
       "\n",
       "            zipcode          lat         long  sqft_living15     sqft_lot15  \n",
       "count   3164.000000  3164.000000  3164.000000    3164.000000    3164.000000  \n",
       "mean   98077.125158    47.557868  -122.212337    1982.544564   13176.302465  \n",
       "std       54.172937     0.140789     0.139577     686.256670   25413.180755  \n",
       "min    98001.000000    47.177500  -122.514000     620.000000     660.000000  \n",
       "25%    98032.000000    47.459575  -122.324250    1480.000000    5429.500000  \n",
       "50%    98059.000000    47.572500  -122.226000    1830.000000    7873.000000  \n",
       "75%    98117.000000    47.680250  -122.124000    2360.000000   10408.250000  \n",
       "max    98199.000000    47.777600  -121.315000    5790.000000  425581.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n",
    "df = pd.read_csv('kc_house_data.csv', sep = ',')\n",
    "\n",
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the feature matrix and the vector of target values. We want to predict the price by using features other than id as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data: 3164\n"
     ]
    }
   ],
   "source": [
    "Data = df.values\n",
    "# m = number of input samples\n",
    "m = Data.shape[0]\n",
    "print(\"Amount of data:\",m)\n",
    "Y = Data[:m,2]\n",
    "X = Data[:m,3:]\n",
    "\n",
    "feature_names = df.columns[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the $m$ samples of the data into 3 parts: one will be used for training and choosing the parameters, one for choosing among different models, and one for testing. The part for training and choosing the parameters will consist of $m_{train}=2/3 m$ samples, the one for choosing among different models will consist of $m_{val}= (m - m_{train})/2$ samples, while the other part consists of $m_{test}=m - m_{train} - m_{val}$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of data for training and deciding parameters: 2109\n",
      "Amount of data for validation (choosing among different models): 527\n",
      "Amount of data for test: 528\n"
     ]
    }
   ],
   "source": [
    "# Split data into train (2/3 of samples), validation (1/6 of samples), and test data (the rest)\n",
    "m_train = int(2./3.*m)\n",
    "m_val = int((m-m_train)/2.)\n",
    "m_test = m - m_train - m_val\n",
    "print(\"Amount of data for training and deciding parameters:\",m_train)\n",
    "print(\"Amount of data for validation (choosing among different models):\",m_val)\n",
    "print(\"Amount of data for test:\",m_test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Xtrain_and_val, Ytrain_and_val is the part of data for training and validation\n",
    "#Xtest, Ytest is the part of data for testing\n",
    "Xtrain_and_val, Xtest, Ytrain_and_val, Ytest = train_test_split(X, Y, test_size=m_test/m, random_state=numero_di_matricola)\n",
    "\n",
    "#if you need to consider a specific training and validation split, use\n",
    "#Xtrain, Ytrain for training and Xval, Yval for validation\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain_and_val, Ytrain_and_val, test_size=m_val/(m_train+m_val), random_state=numero_di_matricola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(Xtrain)\n",
    "Xtrain_scaled = scaler.transform(Xtrain)\n",
    "Xtrain_and_val_scaled = scaler.transform(Xtrain_and_val)\n",
    "Xval_scaled = scaler.transform(Xval)\n",
    "Xtest_scaled = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn the best neural network with 1 hidden layer and between 1 and 9 hidden nodes, choosing the best number of hidden nodes with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    4.3s finished\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=MLPRegressor(),\n",
       "             param_grid={'activation': ['relu'],\n",
       "                         'hidden_layer_sizes': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'random_state': [2019296], 'solver': ['lbfgs']},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp_cv = MLPRegressor()\n",
    "param_grid = {'hidden_layer_sizes': [i for i in range(1,10)],\n",
    "              'activation': ['relu'],\n",
    "              'solver': ['lbfgs'], \n",
    "              'random_state': [numero_di_matricola]\n",
    "             }\n",
    "mlp_GS = GridSearchCV(mlp_cv, param_grid=param_grid, \n",
    "                   cv=5, verbose=True)\n",
    "mlp_GS.fit(Xtrain_and_val_scaled, Ytrain_and_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what is the best parameter, and compare the best NNs with the linear model (learned on train and validation) on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:  MLPRegressor(hidden_layer_sizes=9, random_state=2019296, solver='lbfgs')\n",
      "Error (1-R^2) of best model:  0.18669712680190176\n"
     ]
    }
   ],
   "source": [
    "#let's print the best model according to grid search\n",
    "print(\"Best model: \",mlp_GS.best_estimator_)\n",
    "#let's print the error 1-R^2 for the best model\n",
    "print(\"Error (1-R^2) of best model: \",1. - mlp_GS.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn the best NN using all of training and validation, and then compare the error of the best NN on train and validation and on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error best model on train and validation:  0.10624269122367258\n",
      "Error best model on test data:  0.1582682970296665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "best_mlp = MLPRegressor(hidden_layer_sizes=(9,), activation='relu', solver='lbfgs', random_state = numero_di_matricola)\n",
    "best_mlp.fit(Xtrain_and_val_scaled,Ytrain_and_val)\n",
    "\n",
    "print(\"Error best model on train and validation: \",1. - best_mlp.score(Xtrain_and_val_scaled,Ytrain_and_val))\n",
    "print(\"Error best model on test data: \",1. - best_mlp.score(Xtest_scaled,Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn the linear model on train and validation, and get error (1-R^2) on train and validation and on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - coefficient of determination on training data:0.27231941599689335\n",
      "1 - coefficient of determination on test data:0.3207570465807096\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "#LR the linear regression model\n",
    "LR = linear_model.LinearRegression()\n",
    "\n",
    "#fit the model on training data\n",
    "LR.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
    "\n",
    "print(\"1 - coefficient of determination on training data:\"+str(1 - LR.score(Xtrain_and_val_scaled,Ytrain_and_val)))\n",
    "print(\"1 - coefficient of determination on test data:\"+str(1 - LR.score(Xtest_scaled,Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now explore the k-Nearest Neighbours (kNN) method for regression. In order to do this, you will need to use load the scikit-learn package *neighbors.KNeighborsRegressor* \n",
    "\n",
    "k-Nearest Neighbours for regression works as follows: the predicted value $h(\\textbf{x})$ for an instance $\\textbf{x}$ is obtained by first finding the $\\ell$ instances *in the training set* that are clostest to $\\textbf{x}$; the predicted value $h(\\textbf{x})$ is then the mean of the targets of such $\\ell$ instances. $\\ell$ is a parameter of the method. The targets of the $\\ell$ instances used for prediction can be weighted by the (inverse of) their distance to $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: load the package for kNN regression, learn the model with default parameters using the training and validation scaled data, and print the error (1-R^2) on the data used to train the model and on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on training and validation data:0.15155035705730469\n",
      "Error on test data:0.30508836334396894\n"
     ]
    }
   ],
   "source": [
    "#TO DO: import package\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#TO DO: learn model\n",
    "neigh = KNeighborsRegressor()\n",
    "neigh.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
    "\n",
    "print(\"Error on training and validation data:\"+str(1 - neigh.score(Xtrain_and_val_scaled, Ytrain_and_val)))\n",
    "print(\"Error on test data:\"+str(1 - neigh.score(Xtest_scaled,Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: repeat the point (including the printing instructions) above using the kNN version where points are weighted by the inverse of their distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on training and validation data:0.0006819261974133628\n",
      "Error on test data:0.2985873511140369\n"
     ]
    }
   ],
   "source": [
    "neigh = KNeighborsRegressor(weights='distance')\n",
    "neigh.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
    "\n",
    "print(\"Error on training and validation data:\"+str(1 - neigh.score(Xtrain_and_val_scaled, Ytrain_and_val)))\n",
    "print(\"Error on test data:\"+str(1 - neigh.score(Xtest_scaled,Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: use cross validation to choose the best number of neighbours between 2 and 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(),\n",
       "             param_grid={'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                         14, 15, 16, 17, 18, 19],\n",
       "                         'weights': ['uniform', 'distance']})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_cv_kNN = KNeighborsRegressor()\n",
    "param_grid_kNN = {'n_neighbors': [i for i in range(2,20)],\n",
    "              'weights': ['uniform','distance'],\n",
    "             }\n",
    "mlp_GS_kNN = GridSearchCV(mlp_cv_kNN, param_grid=param_grid_kNN, \n",
    "                   cv=5)\n",
    "mlp_GS_kNN.fit(Xtrain_and_val_scaled, Ytrain_and_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: print the best model according to cross validation above, and print the score of the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model:  KNeighborsRegressor(n_neighbors=7, weights='distance')\n",
      "Score of best model:  0.22498547697709503\n"
     ]
    }
   ],
   "source": [
    "#let's print the best model according to grid search\n",
    "print(\"Best model: \", mlp_GS_kNN.best_estimator_)\n",
    "#let's print the error 1-R^2 for the best model\n",
    "print(\"Score of best model: \", 1 - mlp_GS_kNN.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: learn the best model on all of the training and validation scaled data, and print the error on training and validation scaled data, and on test scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error best model on train and validation: 0.0006819261974133628\n",
      "Error best model on test data: 0.29458714501200967\n"
     ]
    }
   ],
   "source": [
    "#TO DO: learn model\n",
    "best_neigh = KNeighborsRegressor(n_neighbors=7, weights= 'distance')\n",
    "best_neigh.fit(Xtrain_and_val_scaled, Ytrain_and_val)\n",
    "\n",
    "print(\"Error best model on train and validation: \"+str(1 - best_neigh.score(Xtrain_and_val_scaled, Ytrain_and_val)))\n",
    "print(\"Error best model on test data: \"+str(1 - best_neigh.score(Xtest_scaled,Ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error on test data of the best kNN model with the error on test data of linear regression and of NNs. Describe what you observe and give a potential explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error on test data show that the best result is obtained by the NN model. That is not surprising since NNs are very powerful models.Then kNN and linear regression has similar values on the error, the models are not very precise. For the kNN model, in particular, the error on train and validation is very low instead of the test error; that can be a problem of overfitting. Another potential explanation can be that since kNN is a very \"sensitive\" model it may requires a very \"precise\" data set with small number of outliers or no missing data. Linear regression provides the worst results in both training and validation and test error. LR is a simple model and computationally efficient, but in complex analysis can be too simplistic, and, as we can see here, it provides bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and \"Local\" Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to explore the use of clustering to identify groups of *similar* instances, and then learning models that are specific to each group.\n",
    "\n",
    "Once you have clustered the data, and then learned a model for each cluster, the prediction for a new instance is obtained by using the model of the cluster that is the closest to the instance, where the distance of a cluster to the instance is defined as the distance of the *center* of the cluster to the instance.\n",
    "\n",
    "**Note**: in this part you are not explicitely told which part of the data to use, deciding which one is the correct one is part of the homework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: use k-means in sklearn to learn a cluster with 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, random_state=2019296)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans =  KMeans(n_clusters=5, random_state = numero_di_matricola)\n",
    "kmeans.fit(Xtrain_and_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: for each cluster, learn a linear model using the elements of the cluster. For each model, print the error on the data used to learn it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - coefficient of determination on training data for cl1:0.34209117027722935\n",
      "1 - coefficient of determination on training data for cl2:0.3321681868190285\n",
      "1 - coefficient of determination on training data for cl3:0.22987056395499317\n",
      "1 - coefficient of determination on training data for cl4:0.09407429231926945\n",
      "1 - coefficient of determination on training data for cl5:0.35851920310574426\n"
     ]
    }
   ],
   "source": [
    "#To learn the linear models I divide the elements of each cluster and the respective label.\n",
    "\n",
    "cl1_i = []\n",
    "cl2_i = []\n",
    "cl3_i = []\n",
    "cl4_i = []\n",
    "cl5_i = []\n",
    "for i in range(0,len(kmeans.labels_)):\n",
    "    if kmeans.predict(Xtrain_and_val_scaled)[i]==0:\n",
    "        cl1_i.append(i)\n",
    "        cl1 = list(Xtrain_and_val_scaled[cl1_i])\n",
    "        y_cl1 = list(Ytrain_and_val[cl1_i])\n",
    "    elif kmeans.predict(Xtrain_and_val_scaled)[i]==1:\n",
    "        cl2_i.append(i)\n",
    "        cl2 = list(Xtrain_and_val_scaled[cl2_i])\n",
    "        y_cl2 = list(Ytrain_and_val[cl2_i])\n",
    "    elif kmeans.predict(Xtrain_and_val_scaled)[i]==2:\n",
    "        cl3_i.append(i)\n",
    "        cl3 = list(Xtrain_and_val_scaled[cl3_i])\n",
    "        y_cl3 = list(Ytrain_and_val[cl3_i])\n",
    "    elif kmeans.predict(Xtrain_and_val_scaled)[i]==3:\n",
    "        cl4_i.append(i)\n",
    "        cl4 = list(Xtrain_and_val_scaled[cl4_i])\n",
    "        y_cl4 = list(Ytrain_and_val[cl4_i])\n",
    "    elif kmeans.predict(Xtrain_and_val_scaled)[i]==4:\n",
    "        cl5_i.append(i)\n",
    "        cl5 = list(Xtrain_and_val_scaled[cl5_i])\n",
    "        y_cl5 = list(Ytrain_and_val[cl5_i])\n",
    "        \n",
    "\n",
    "\n",
    "#I fit the five models\n",
    "LR_Cl1 = linear_model.LinearRegression().fit(cl1, y_cl1)\n",
    "LR_Cl2 = linear_model.LinearRegression().fit(cl2, y_cl2)\n",
    "LR_Cl3 = linear_model.LinearRegression().fit(cl3, y_cl3)\n",
    "LR_Cl4 = linear_model.LinearRegression().fit(cl4, y_cl4)\n",
    "LR_Cl5 = linear_model.LinearRegression().fit(cl5, y_cl5)\n",
    "\n",
    "\n",
    "print(\"1 - coefficient of determination on training data for cl1:\"+str(1 - LR_Cl1.score(cl1, y_cl1)))\n",
    "print(\"1 - coefficient of determination on training data for cl2:\"+str(1 - LR_Cl2.score(cl2, y_cl2)))\n",
    "print(\"1 - coefficient of determination on training data for cl3:\"+str(1 - LR_Cl3.score(cl3, y_cl3)))\n",
    "print(\"1 - coefficient of determination on training data for cl4:\"+str(1 - LR_Cl4.score(cl4, y_cl4)))\n",
    "print(\"1 - coefficient of determination on training data for cl5:\"+str(1 - LR_Cl5.score(cl5, y_cl5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: *compute* the error (1 - R^2) on the data not used to learn the models.\n",
    "For each instance not used to learn the model, the prediction is done by:\n",
    "- finding the cluster C whose center is the closest to the instance\n",
    "- use the model learned for cluster C to make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using transform() from kmeans it returns the distance from all the centers.\n",
    "kmeans.transform(Xtest_scaled)\n",
    "lab_idx =np.argmin(kmeans.transform(Xtest_scaled), axis=1)\n",
    "tcl1_i = []\n",
    "tcl2_i = []\n",
    "tcl3_i = []\n",
    "tcl4_i = []\n",
    "tcl5_i = []\n",
    "for i in range(0,len(Xtest_scaled)):\n",
    "    if lab_idx[i]==0:\n",
    "        tcl1_i.append(i)\n",
    "        cl1t = list(Xtest_scaled[tcl1_i])\n",
    "        y_cl1t = list(Ytest[tcl1_i])\n",
    "    elif lab_idx[i]==1:\n",
    "        tcl2_i.append(i)\n",
    "        cl2t = list(Xtest_scaled[tcl2_i])\n",
    "        y_cl2t = list(Ytest[tcl2_i])\n",
    "    elif lab_idx[i]==2:\n",
    "        tcl3_i.append(i)\n",
    "        cl3t = list(Xtest_scaled[tcl3_i])\n",
    "        y_cl3t = list(Ytest[tcl3_i])\n",
    "    elif lab_idx[i]==3:\n",
    "        tcl4_i.append(i)\n",
    "        cl4t = list(Xtest_scaled[tcl4_i])\n",
    "        y_cl4t = list(Ytest[tcl4_i])\n",
    "    elif lab_idx[i]==4:\n",
    "        tcl5_i.append(i)\n",
    "        cl5t = list(Xtest_scaled[tcl5_i])\n",
    "        y_cl5t = list(Ytest[tcl5_i])\n",
    "#Now I can make the prediction using the appropriate model:\n",
    "predC1 = LR_Cl1.predict(cl1t)\n",
    "predC2 = LR_Cl2.predict(cl2t)\n",
    "predC3 = LR_Cl3.predict(cl3t)\n",
    "predC4 = LR_Cl4.predict(cl4t)\n",
    "predC5 = LR_Cl5.predict(cl5t)\n",
    "\n",
    "pred_test = np.concatenate((predC1,predC2,predC3,predC4,predC5))\n",
    "truelab = np.concatenate((y_cl1t,y_cl2t,y_cl3t,y_cl4t,y_cl5t))\n",
    "\n",
    "#Finally I can compute the R^2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2error = r2_score(truelab,pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: *print* the error (1-R^2) on the data not used to learn the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - coefficient of determination on the data not used to learn the model:0.216981135776511\n"
     ]
    }
   ],
   "source": [
    "print(\"1 - coefficient of determination on the data not used to learn the model:\"+str(1 - r2error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error of the model \"clustering + linear models\" and of the linear model (see the beginning of the HW). Describe what you observe, and provide a possible explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using clustering + linear models I have obtained an improvement on the error results, expecially on the test data with respect to linear regression. A possibile explanation is that clustering the data, i.e. group a set of objects such that similar objects ends up in the same group and dissimilar objects sare separated into different groups, helped to simplify the models that linear regression had to learn. In fact, as I wrote before (in kNN answer) linear regression has limits about the complexity of the model to learn, but in this case clustering helped to overwhelm them, producing better resuls then linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error of the model \"clustering + linear models\" and of kNN. Describe what you observe, and provide a possible explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have opposite situations: for kNN I have obtained a very good result on the training and validation set but for the test set the result was not good; for the clustering + linear models I have obtained the opposite. A possibile explanation is that for both model the training data were too much. In particular for kNN I have a possibile overfitting. For clustering + linear model probably I have too many features that instead of help, they damage the model. Maybe a solution will be to use only the features that are more correlated with the price of the houses in order to simplify the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and \"Local\" NNs\n",
    "\n",
    "Repeat the same as above, but using neural networks instead of linear models.\n",
    "\n",
    "**Note**: note that we are not telling you which parameters to use for NNs. You have to decide how to select the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: clearly explain how you decided to set the parameters, motivating the choice of your strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used grid search CV to find the best parameters for each cluster. I tried hidden_layer_sizes from 1 to 50, with activation 'relu' and solver 'lbfgs' and 'adam'. The results I have obtained are:\n",
    "Cluster 1 (label 0) : hidden_layer_sizes = 34, activation 'relu' and solver 'lbfgs';\n",
    "Cluster 2 (label 1) : hidden_layer_sizes = 4, activation 'relu' and solver 'lbfgs';\n",
    "Cluster 3 (label 2) : hidden_layer_sizes = 18, activation 'relu' and solver 'lbfgs';\n",
    "Cluster 4 (label 3) : hidden_layer_sizes = 2, activation 'relu' and solver 'lbfgs';\n",
    "Cluster 5 (label 4) : hidden_layer_sizes = 14, activation 'relu' and solver 'lbfgs';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: repeat the analysis in part \"Clustering and \"Local\" Linear Models\" using NNs instead of linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of best model for cluster 1:0.19731398468779693\n",
      "Error of best model for cluster 2:0.25304286905539275\n",
      "Error of best model for cluster 3:0.14236998171344473\n",
      "Error of best model for cluster 4:0.09407429261210098\n",
      "Error of best model for cluster 5:0.261371722470109\n",
      "Error on the data not used to learn the model:0.20533294884865916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolafarronato/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "#I fit the five models using the best parameters founded with grid search CV.\n",
    "best_mlp_cl1 = MLPRegressor(hidden_layer_sizes=(34,), activation='relu', solver='lbfgs', random_state = numero_di_matricola).fit(cl1, y_cl1)\n",
    "best_mlp_cl2 = MLPRegressor(hidden_layer_sizes=(4,), activation='relu', solver='lbfgs', random_state = numero_di_matricola).fit(cl2, y_cl2)\n",
    "best_mlp_cl3 = MLPRegressor(hidden_layer_sizes=(18,), activation='relu', solver='lbfgs', random_state = numero_di_matricola).fit(cl3, y_cl3)\n",
    "best_mlp_cl4 = MLPRegressor(hidden_layer_sizes=(2,), activation='relu', solver='lbfgs', random_state = numero_di_matricola).fit(cl4, y_cl4)\n",
    "best_mlp_cl5 = MLPRegressor(hidden_layer_sizes=(14,), activation='relu', solver='lbfgs', random_state = numero_di_matricola).fit(cl5, y_cl5)\n",
    "\n",
    "print(\"Error of best model for cluster 1:\"+str(1 - best_mlp_cl1.score(cl1, y_cl1)))\n",
    "print(\"Error of best model for cluster 2:\"+str(1 - best_mlp_cl2.score(cl2, y_cl2)))\n",
    "print(\"Error of best model for cluster 3:\"+str(1 - best_mlp_cl3.score(cl3, y_cl3)))\n",
    "print(\"Error of best model for cluster 4:\"+str(1 - best_mlp_cl4.score(cl4, y_cl4)))\n",
    "print(\"Error of best model for cluster 5:\"+str(1 - best_mlp_cl5.score(cl5, y_cl5)))\n",
    "\n",
    "mlp_predC1 = best_mlp_cl1.predict(cl1t)\n",
    "mlp_predC2 = best_mlp_cl2.predict(cl2t)\n",
    "mlp_predC3 = best_mlp_cl3.predict(cl3t)\n",
    "mlp_predC4 = best_mlp_cl4.predict(cl4t)\n",
    "mlp_predC5 = best_mlp_cl5.predict(cl5t)\n",
    "\n",
    "mlp_pred_test = np.concatenate((mlp_predC1,mlp_predC2,mlp_predC3,mlp_predC4,mlp_predC5))\n",
    "mlp_truelab = np.concatenate((y_cl1t,y_cl2t,y_cl3t,y_cl4t,y_cl5t))\n",
    "\n",
    "#Finally I can compute the R^2 score\n",
    "from sklearn.metrics import r2_score\n",
    "r2error = r2_score(mlp_truelab,mlp_pred_test)\n",
    "print(\"Error on the data not used to learn the model:\"+str(1 - r2error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error of the model \"clustering + NNs\" and of NNs (see the beginning of the HW). Describe what you observe, and provide a possible explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, using clustering + NNs, I have obtained worst results than the NNs only. Since NNs are a very powerful models a possibile explanation is that the clusters are not good enough. There are many variables that were not taken into account in my analysis, like the number of clusters equal to 5. Perhaps deeper analysis might say that the optimal number is different. Also the number of featured and the number of samples for the training and test can be a problem of those results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error of the model \"clustering + NNs\" and of kNN. Describe what you observe, and provide a possible explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of the model kNN has a good result on training and validation instead of the error for the test data. For the model clustering + NNs I did not find optimal values, neither in training nor in test, for the error. If I have to choose one I woud pick clustering + NNs because the error on the test data is foundamental for choosing a model in real machine learning analisys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: compare the error of the model \"clustering + NNs\" and of \"clustering + Linear Models\". Describe what you observe, and provide a possible explanation.\n",
    "## [USE MAX 10 LINES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of the model clustering + NNs and clustering + linear model are very close if we see it on the test data. For the training data NNs performs better, as one can expect. In this case the errors have close values, instead of using NNs and linear regression(without clustering), where NNs clearly exceeded the results of linear regression model. A possible explanation could be that the cluster analysis I have performed \"facilitated\" linear models instead of NNs. Probably using different techiques (as I have proposed on last aswers) could led to different results, expecially for NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
